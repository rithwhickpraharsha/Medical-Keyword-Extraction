# -*- coding: utf-8 -*-
"""SGD_Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ecZQ7OoPxaSFrbD0Dg1oE2pUDH4pB9hq
"""

!pip install keras

!pip install tensorflow

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import seaborn as sns
import matplotlib.pyplot as plt
import itertools
import math
import pandas as pd
import numpy as np
import re
import joblib
# preprocessing
import os
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.utils import to_categorical

from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfTransformer
# from sklearn.externals import joblib
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.multiclass import OneVsRestClassifier
from sklearn.linear_model import SGDClassifier
from sklearn.metrics import accuracy_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
from sklearn.metrics import precision_score
from sklearn.metrics import classification_report
from sklearn.metrics import hamming_loss

import nltk
nltk.download('stopwords')
from nltk.tokenize import RegexpTokenizer
from nltk.corpus import stopwords

!pip install wordcloud

from tensorflow.keras.preprocessing.text import Tokenizer

# from google.colab import drive
# drive.mount('/content/drive')

"""### We are loading the dataset"""

import pandas as pd

# Use the Raw URL for the CSV file
url = 'https://raw.githubusercontent.com/Saloni-glit/Medical-dataset/main/mtsamples.csv'
df = pd.read_csv(url)

# Now you should be able to read the CSV file successfully

df.astype(str)
df.head()

df["keywords"].str.split(", ")

"""# Text Preparation Functions

## text_prepare

### Description
This function prepares the input text by performing the following operations:
- Converts the text to lowercase.
- Replaces specific symbols defined by `REPLACE_BY_SPACE_RE` with spaces.
- Deletes symbols matching the pattern defined by `BAD_SYMBOLS_RE` from the text.
- Removes stopwords from the text.

### Parameters
- `text`: Input text to be prepared.



"""

def text_prepare(text):

    REPLACE_BY_SPACE_RE = re.compile('[/(){}\[\]\|@\-,.;#&]')
    BAD_SYMBOLS_RE = re.compile('[0-9][0-9a-z ][#+_]{1,}')
    STOPWORDS = set(stopwords.words('english'))

    text = text.lower()
    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text
    text = BAD_SYMBOLS_RE.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text
    words = text.split()
    i = 0
    while i < len(words):
        if words[i] in STOPWORDS:
            words.pop(i)
        else:
            i += 1
    text = ' '.join(map(str, words))# delete stopwords from text

    return text
def text_prepare_keywords(text):
    REPLACE_BY_SPACE_RE = re.compile('[/(){}\[\]\|@\-.,;#&]')
    BAD_SYMBOLS_RE = re.compile('[0-9][0-9a-z #+%_]')
    STOPWORDS = set(stopwords.words('english'))

    text = text.lower()
    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text
    text = BAD_SYMBOLS_RE.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text
    words = text.split()
    i = 0
    while i < len(words):
        if words[i] in STOPWORDS:
            words.pop(i)
        else:
            i += 1
    text = ' '.join(map(str, words))# delete stopwords from text

    return text

"""
### Replace Empty Strings in 'keywords' Column

### Description
This operation replaces empty strings in the 'keywords' column with NaN values."""

df['keywords'].replace('', np.nan, inplace=True)
df = df.drop(df[df['transcription'].isna()].index)
df = df.drop(df[df['keywords'].isna()].index)

df.shape

# df['transcription'] = df['transcription'].str.replace(r'[0-9-A-Z\s]+:', '')
# df['keywords'] = df['keywords'].str.replace(r'[0-9-A-Z\s]+:', '')
# df['keywords'] = df['keywords'].str.replace(r'(, *)?$', '')
# df['keywords'] = df['keywords'].str.replace(r'^( *,)?', '')
# df['keywords'] = df['keywords'].str.replace(r'\s{2,}', '')
# df['keywords'] = df['keywords'].str.replace(r'%', '')
# df.loc[:, 'transcription'] = [text_prepare(x) for x in df['transcription'].values]
# df.loc[:, 'keywords'] = [text_prepare_keywords(x) for x in df['keywords'].values]
df['transcription'] = df['transcription'].str.replace(r'[0-9-A-Z\s]+:', '', regex=True)
df['keywords'] = df['keywords'].str.replace(r'[0-9-A-Z\s]+:', '', regex=True)
df['keywords'] = df['keywords'].str.replace(r'(, *)?$', '', regex=True)
df['keywords'] = df['keywords'].str.replace(r'^( *,)?', '', regex=True)
df['keywords'] = df['keywords'].str.replace(r'\s{2,}', '', regex=True)
df['keywords'] = df['keywords'].str.replace(r'%', '', regex=True)
df.loc[:, 'transcription'] = [text_prepare(x) for x in df['transcription'].values]
df.loc[:, 'keywords'] = [text_prepare_keywords(x) for x in df['keywords'].values]

"""### Tokenization of 'keywords' Column

### Tokenize 'keywords' Column and Create 'tokens_keywords' Column

### Description
This operation tokenizes the 'keywords' column and creates a new column 'tokens_keywords' containing the tokenized words.

"""

tokenizer = RegexpTokenizer(r'[\w\'-]+')
df["tokens_keywords"] = df["keywords"].apply(tokenizer.tokenize)
df["tokens_keywords"].head()

df["keyword_count"] = df['keywords'].apply(lambda text: len(text.split(" ")))
df.head()

"""
### Description
This section provides the counts of unique values in the 'keyword_count' column."""

df.keyword_count.value_counts()

"""
## Description
This section applies Count Vectorizer to convert the 'keywords' column into a document-term matrix (DTM).

### Code
```python
from sklearn.feature_extraction.text import CountVectorizer

# Initialize Count Vectorizer with a custom tokenizer
vectorizer = CountVectorizer(tokenizer=lambda x: x.split(' '))

# Fit and transform the 'keywords' column
keywords_dtm = vectorizer.fit_transform(df['keywords'])

# Get feature names (tags) from the vectorizer
tags = vectorizer.get_feature_names_out()
"""

# using count vectorizer
# vectorizer = CountVectorizer(tokenizer = lambda x: x.split(' '))

# keywords_dtm = vectorizer.fit_transform(df['keywords'])
vectorizer = CountVectorizer(tokenizer=lambda x: x.split(' '))
keywords_dtm = vectorizer.fit_transform(df['keywords'])
tags = vectorizer.get_feature_names_out()

keywords_dtm

print("Number of data points :", keywords_dtm.shape[0])
print("Number of unique tags :", keywords_dtm.shape[1])

"""# Storing the Count of Tags in Each Transcription

In this section, we calculate the count of tags in each transcription and convert the resulting list into a single list.



"""

# Storing the count of tag in each transcription in the list 'tag_count
tag_quest_count = keywords_dtm.sum(axis=1).tolist()

# converting list of lists into single list,we will get [[3], [4], [2], [2], [3]] and we are converting this to [3, 4, 2, 2, 3]
tag_quest_count=[int(j) for i in tag_quest_count for j in i]
print('We have total {} datapoints.'.format(len(tag_quest_count)))
print(tag_quest_count[:5])

df['tag_quest_count']=tag_quest_count

df

print ("Maximum no of tag per transcription: %d"%max(tag_quest_count))
print ("Minimum no of tags per transcription: %d"%min(tag_quest_count))
print ("Avg number of tags per transcription: %f"% ((sum(tag_quest_count)*1.0)/len(tag_quest_count)))

sns.set_palette("gist_rainbow")

# Plotting the bar graph
plt.figure(figsize=(20, 10))
sns.barplot(x=df.index[:90], y=df['tag_quest_count'][:90])
plt.title("Tag Quest Count in Each Row")
plt.xlabel("Row Number")
plt.ylabel("Tag Quest Count")
plt.show()

"""# Calculating Tag Frequencies in Document Term Matrix (DTM)

In this section, we calculate the frequencies of tags in the Document Term Matrix (DTM) and store the results in a dictionary.



"""

#Lets now store the document term matrix in a dictionary.


freqs = keywords_dtm.sum(axis=0).A1
result = dict(zip(tags, freqs))

keywords_dtm

# Creating a DataFrame with Tags and their Counts
tag_df = pd.DataFrame({'Tags': tags, 'Counts': freqs})
tag_df.head()

tag_df_sorted = tag_df.sort_values(['Counts'], ascending=False)
tag_counts = tag_df_sorted['Counts'].values

plt.plot(tag_counts[:1000])
plt.title("Distribution of number of times tag appeared")
plt.grid()
plt.xlabel("Tag number")
plt.ylabel("Number of times tag appeared")
plt.show()

plt.plot(tag_counts, c='b')
plt.scatter(x=list(range(0,100,5)), y=tag_counts[0:100:5], c='orange', label="quantiles with 0.05 intervals")
# quantiles with 0.25 difference
plt.scatter(x=list(range(0,100,25)), y=tag_counts[0:100:25], c='m', label = "quantiles with 0.25 intervals")

for x,y in zip(list(range(0,100,25)), tag_counts[0:100:25]):
#     plt.annotate(s="({} , {})".format(x,y), xy=(x,y), xytext=(x-0.05, y+500))
     plt.annotate(text="({} , {})".format(x,y), xy=(x,y), xytext=(x-0.05, y+500))

plt.title('first 100 tags: Distribution of number of times tag appeared transcription')
plt.grid()
plt.xlabel("Tag number")
plt.ylabel("Number of times tag appeared")
plt.legend()
plt.show()
print(len(tag_counts[0:100:5]), tag_counts[0:100:5])

"""### Lets first convert the 'result' dictionary to 'list of tuples' ,Initializing Word Cloud using frequencies of tags.

A word cloud is a data visualization technique used to represent text data in which the size of each word indicates its frequency or importance. In a word cloud, more frequently occurring words are displayed in larger and bolder fonts. It's a way to visually explore and represent the most prominent terms in a corpus of text.

"""

import datetime
from wordcloud import WordCloud
# Ploting word cloud
start = datetime.datetime.now()

# Lets first convert the 'result' dictionary to 'list of tuples'
tup = dict(result.items())
#Initializing WordCloud using frequencies of tags.
wordcloud = WordCloud(    background_color='black',
                          width=1600,
                          height=800,
                    ).generate_from_frequencies(tup)

fig = plt.figure(figsize=(30,20))
plt.imshow(wordcloud)
plt.axis('off')
plt.tight_layout(pad=0)
fig.savefig("tag.png")
plt.show()
print("Time taken to run this cell :", datetime.datetime.now() - start)

# Obsetving the quantiles using the violin plot and box .

plt.figure(figsize=(10, 8))

plt.subplot(1,2,1)
sns.violinplot(data = tag_df_sorted.head(30) , )
plt.xlabel("Number of Tags")
plt.ylabel("Number of transcription")

plt.subplot(1,2,2)
sns.boxplot(tag_quest_count, palette='gist_rainbow')
plt.xlabel("Number of Tags")
plt.ylabel("Number of transcription")
plt.show()

df.drop(['description', 'medical_specialty', 'sample_name', 'tokens_keywords'], axis=1, inplace=True)

df.head()

"""# Splitting the DataFrame into Train and Test Sets

We use the `sample` method to split the DataFrame `df` into training and testing sets.

This code randomly samples 50% of the data for training (x_train) and 20% for testing (x_test). The frac parameter controls the fraction of data to be sampled, and replace=True allows for replacement during sampling. The random_state ensures reproducibility of the split

x_train=df.sample(frac=0.5, replace=True, random_state=1)
x_test=df.sample(frac=0.2, replace=True, random_state=1)

## Analyzing Keyword Frequencies in Training Data

We use the Natural Language Toolkit (nltk) and seaborn libraries to analyze the frequency distribution of keywords in the training data.

### Calculate the frequency distribution of the first word in each keyword

### Create a DataFrame to store Keywords and their Counts

### Select the top 100 keywords based on their frequency
g = all_genres_df.nlargest(columns="Count", n=100)

### Plot a bar chart to visualize keyword frequencies

### Display a list of all unique keywords
list_of_keywords = list(all_genres.keys())
"""

import nltk
import seaborn as sns
all_genres = nltk.FreqDist(x_train["keywords"].apply(lambda x: x.split(' ')[0]))
all_genres_df = pd.DataFrame({'Keywords': list(all_genres.keys()), 'Count': list(all_genres.values())})
g = all_genres_df.nlargest(columns="Count", n = 100)
plt.figure(figsize=(12,15))
ax = sns.barplot(data=g, x= "Count", y = "Keywords")
ax.set(ylabel = 'Count')
plt.show()
list(all_genres.keys())

"""# Text Classification Model Training using scikit-learn Pipeline

In this code snippet, we demonstrate the training of a text classification model using scikit-learn's Pipeline. The model is a combination of a CountVectorizer, TfidfTransformer, and a OneVsRestClassifier with Stochastic Gradient Descent (SGD) as the base classifier.

1. **Data Preparation:**
   - You split your dataset into training and test sets using the `sample` method, ensuring random sampling and replacement for both sets.
#CountVectorizer : Converts a collection of text documents to into a bag-of-words representation,  
#TfidfTransformer: Transforms a count matrix to a normalized term-frequency or term-frequency times inverse document-frequency representation.
#OneVsRestClassifier:This a strategy for multi-label classification in which a separate classifier is trained for each label.

"""

from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfTransformer

# You are extracting the 'transcription' and 'keywords' columns from your training and test datasets and converting them into NumPy arrays.
X_train = np.array(x_train['transcription'])
Y_train = np.array(x_train['keywords'])
X_test = np.array(x_test['transcription'])
Y_test = np.array(x_test['keywords'])

model = Pipeline([('vect', CountVectorizer()),
                      ('tfidf', TfidfTransformer()),
                      ('clf', OneVsRestClassifier(SGDClassifier(loss='log', penalty='l1', class_weight="balanced", verbose=1), n_jobs=-1))
                      ])
model.fit(X_train, Y_train)

"""
### We save our trained model using the joblib.dump function. The model is serialized and stored in a file named "pipeline1.pkl" with compression level 9."""

# from sklearn.externals
# This line saves your trained model using the joblib.dump function. The model is serialized and stored in a file named "pipeline1.pkl" with compression level 9.
import joblib
joblib.dump(model, "pipeline1.pkl", compress=9)
prediction = model.predict(X_test)

"""#      RESULTS"""

print("Accuracy :",accuracy_score(Y_test, prediction))
print("Hamming loss ",hamming_loss(Y_test,prediction))


precision = precision_score(Y_test, prediction, average='micro')
recall = recall_score(Y_test, prediction, average='micro')
f1 = f1_score(Y_test, prediction, average='micro')

print("Micro-average quality numbers")
print("Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}".format(precision, recall, f1))

precision = precision_score(Y_test, prediction, average='macro')
recall = recall_score(Y_test, prediction, average='macro')
f1 = f1_score(Y_test, prediction, average='macro')

print("Macro-average quality numbers")
print("Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}".format(precision, recall, f1))

print('Input: '+X_test[1])
prediction_row = model.predict([X_test[1]])
print('Output: '+prediction_row[0])
print('Actual: '+Y_test[1])

